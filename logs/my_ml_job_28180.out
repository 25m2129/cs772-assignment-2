Train path: True
Valid path: True
Test path : True
Train: 1299155  Valid: 6357  Test: 10112
Train size after length-stratified subsampling: 100000
Saved cleaned & stratified data to data/processed
English char vocab size: 26
Native  char vocab size: 66

Top 20 English chars:
[('a', 174154), ('i', 64930), ('n', 60393), ('r', 57919), ('h', 56822), ('e', 47469), ('t', 42851), ('s', 42631), ('o', 34234), ('d', 31456), ('l', 31422), ('u', 29478), ('k', 29031), ('m', 27349), ('p', 21835), ('y', 19527), ('c', 19381), ('g', 19022), ('b', 18453), ('v', 15512)]

Top 20 Native chars:
[('ा', 71943), ('र', 56308), ('्', 54763), ('ि', 35941), ('न', 32886), ('क', 32080), ('स', 31604), ('े', 30752), ('ल', 30253), ('ी', 28268), ('म', 26041), ('ं', 24577), ('ो', 22760), ('व', 22173), ('त', 21327), ('प', 20757), ('य', 20681), ('ु', 17553), ('ग', 15832), ('ट', 15724)]
Device: cuda
Vocab size (incl specials): 98
Example chars: ['<pad>', '<sos>', '<eos>', '<unk>', 'a', 'ा', 'i', 'n', 'r', 'h', 'र', '्', 'e', 't', 's', 'ि', 'o', 'न', 'क', 'स']
Train batches: 782 Valid batches: 50 Test batches: 79
Transformer params: 2711138
No checkpoint found at models/transformer_trf.pth. Starting training for 12 epochs.

=== TRF Epoch 1/12 ===
Train loss: 1.4478  Val loss: 0.6516
Val exact_acc: 0.2109  char_acc: 0.7226  mean_edit: 2.057
Saved best transformer checkpoint.

=== TRF Epoch 2/12 ===
Train loss: 0.7494  Val loss: 0.5300
Val exact_acc: 0.2754  char_acc: 0.7703  mean_edit: 1.527
Saved best transformer checkpoint.

=== TRF Epoch 3/12 ===
Train loss: 0.6068  Val loss: 0.4689
Val exact_acc: 0.3112  char_acc: 0.7970  mean_edit: 1.339
Saved best transformer checkpoint.

=== TRF Epoch 4/12 ===
Train loss: 0.5262  Val loss: 0.4373
Val exact_acc: 0.3381  char_acc: 0.8126  mean_edit: 1.232
Saved best transformer checkpoint.

=== TRF Epoch 5/12 ===
Train loss: 0.4766  Val loss: 0.4172
Val exact_acc: 0.3626  char_acc: 0.8212  mean_edit: 1.197
Saved best transformer checkpoint.

=== TRF Epoch 6/12 ===
Train loss: 0.4433  Val loss: 0.4008
Val exact_acc: 0.3744  char_acc: 0.8272  mean_edit: 1.135
Saved best transformer checkpoint.

=== TRF Epoch 7/12 ===
Train loss: 0.4184  Val loss: 0.3912
Val exact_acc: 0.3874  char_acc: 0.8319  mean_edit: 1.107
Saved best transformer checkpoint.

=== TRF Epoch 8/12 ===
Train loss: 0.3999  Val loss: 0.3854
Val exact_acc: 0.3867  char_acc: 0.8324  mean_edit: 1.106
Saved best transformer checkpoint.

=== TRF Epoch 9/12 ===
Train loss: 0.3843  Val loss: 0.3795
Val exact_acc: 0.3989  char_acc: 0.8370  mean_edit: 1.070
Saved best transformer checkpoint.

=== TRF Epoch 10/12 ===
Train loss: 0.3711  Val loss: 0.3711
Val exact_acc: 0.4055  char_acc: 0.8415  mean_edit: 1.041
Saved best transformer checkpoint.

=== TRF Epoch 11/12 ===
Train loss: 0.3599  Val loss: 0.3669
Val exact_acc: 0.4110  char_acc: 0.8433  mean_edit: 1.029
Saved best transformer checkpoint.

=== TRF Epoch 12/12 ===
Train loss: 0.3499  Val loss: 0.3648
Val exact_acc: 0.4134  char_acc: 0.8448  mean_edit: 1.021
Saved best transformer checkpoint.
Loaded TRF checkpoint epoch 12

=== Test (greedy) ===
Exact: 0.3907238924050633 Char acc: 0.8376725242145561 Mean edit: 1.120549841772152

Sample predictions (greedy):
SRC: maitrologist  GOLD: मैट्रोलॉजिस्ट  PRED: मैट्रोलॉजिस्ट
SRC: phwcs  GOLD: पीएचडब्ल्यूसीएस  PRED: फ्वक्स
SRC: pratidwandiyon  GOLD: प्रतिद्वन्दियों  PRED: प्रतिद्वांदियों
SRC: pratiyukti  GOLD: प्रतियुक्ति  PRED: प्रतियुक्ति
SRC: eksisatens  GOLD: एक्सिसटेंस  PRED: एक्सीसेटेंस
SRC: filmnirmata  GOLD: फ़िल्मनिर्माता  PRED: फिल्मनिर्मता
SRC: adrgh  GOLD: अद्र्घ  PRED: अद्र्घ
SRC: ladhege  GOLD: लड़ेगे  PRED: लढेगे
SRC: administresh  GOLD: एडमिनिस्ट्रेश  PRED: अदमिनिस्त्रेश
SRC: shiwalapurwa  GOLD: शिवालापुरवा  PRED: शिवलापुरवा
Total test examples predicted: 10112

Top character confusions (gold -> predicted), sorted by frequency:
'ा'  ->  '<eps>'   count=1374
'्'  ->  '<eps>'   count=843
'<eps>'  ->  'ा'   count=600
'़'  ->  '<eps>'   count=390
'<eps>'  ->  '्'   count=362
'ट'  ->  'त'   count=360
'ी'  ->  'ि'   count=320
'ड'  ->  'द'   count=259
'<eps>'  ->  'े'   count=257
'न'  ->  'ं'   count=207
'ू'  ->  'ु'   count=204
'ि'  ->  'ी'   count=149
'य'  ->  '<eps>'   count=135
'ॉ'  ->  'ो'   count=127
'<eps>'  ->  '़'   count=120
'द'  ->  'ड'   count=118
'े'  ->  '<eps>'   count=114
'ं'  ->  'न'   count=102
'ं'  ->  '<eps>'   count=101
'त'  ->  'ट'   count=97
'<eps>'  ->  'ए'   count=93
'ू'  ->  '<eps>'   count=77
'ट'  ->  '<eps>'   count=77
'<eps>'  ->  'ु'   count=72
'<eps>'  ->  'ं'   count=72
'ल'  ->  '<eps>'   count=66
'ः'  ->  'ह'   count=64
'न'  ->  '<eps>'   count=64
'ु'  ->  'ू'   count=63
'त'  ->  '<eps>'   count=63

Summary of edit operation counts: {'replace': 6930, 'delete': 2935, 'insert': 1773}
Wrote char confusions to error_analysis_char_confusions.csv

Top roman n-grams (n<= 3) by error rate (min support = 20):
n=3  'bly'  support=27  errors=27  err_rate=1.000
n=3  'aik'  support=23  errors=22  err_rate=0.957
n=2  'ff'  support=22  errors=21  err_rate=0.955
n=2  'za'  support=49  errors=45  err_rate=0.918
n=2  'sy'  support=24  errors=22  err_rate=0.917
n=3  'tap'  support=22  errors=20  err_rate=0.909
n=2  'az'  support=54  errors=49  err_rate=0.907
n=3  'abl'  support=32  errors=29  err_rate=0.906
n=3  'vil'  support=32  errors=29  err_rate=0.906
n=3  'mul'  support=21  errors=19  err_rate=0.905
n=3  'wad'  support=27  errors=24  err_rate=0.889
n=3  'ees'  support=35  errors=31  err_rate=0.886
n=3  'dat'  support=26  errors=23  err_rate=0.885
n=2  'ly'  support=64  errors=56  err_rate=0.875
n=3  'iks'  support=23  errors=20  err_rate=0.870
n=1  'z'  support=165  errors=143  err_rate=0.867
n=3  'jya'  support=22  errors=19  err_rate=0.864
n=2  'cu'  support=22  errors=19  err_rate=0.864
n=3  'agr'  support=22  errors=19  err_rate=0.864
n=2  'rl'  support=28  errors=24  err_rate=0.857
n=3  'agh'  support=21  errors=18  err_rate=0.857
n=3  'sid'  support=27  errors=23  err_rate=0.852
n=3  'nal'  support=20  errors=17  err_rate=0.850
n=2  'ef'  support=20  errors=17  err_rate=0.850
n=3  'sto'  support=20  errors=17  err_rate=0.850
n=3  'jar'  support=33  errors=28  err_rate=0.848
n=3  'add'  support=38  errors=32  err_rate=0.842
n=3  'rus'  support=25  errors=21  err_rate=0.840
n=3  'nce'  support=25  errors=21  err_rate=0.840
n=2  'ob'  support=41  errors=34  err_rate=0.829
Wrote roman ngram stats to error_analysis_roman_ngrams.csv

Levenshtein edit distance over test set:
mean: 1.120549841772152 median: 1.0 max: 14

Sample errors (showing up to 30):
SRC: phwcs  GOLD: पीएचडब्ल्यूसीएस  PRED: फ्वक्स
SRC: pratidwandiyon  GOLD: प्रतिद्वन्दियों  PRED: प्रतिद्वांदियों
SRC: eksisatens  GOLD: एक्सिसटेंस  PRED: एक्सीसेटेंस
SRC: filmnirmata  GOLD: फ़िल्मनिर्माता  PRED: फिल्मनिर्मता
SRC: ladhege  GOLD: लड़ेगे  PRED: लढेगे
SRC: administresh  GOLD: एडमिनिस्ट्रेश  PRED: अदमिनिस्त्रेश
SRC: shiwalapurwa  GOLD: शिवालापुरवा  PRED: शिवलापुरवा
SRC: nirmaali  GOLD: निर्माली  PRED: निरमाली
SRC: chootie  GOLD: चूतीए  PRED: चूटी
SRC: kursiyo  GOLD: कुर्सियॉ  PRED: कुरसियो
SRC: enseeaardablyusee  GOLD: एनसीआरडब्ल्यूसी  PRED: एंसीएयरडेब्ल्यूजी
SRC: shivaalapurva  GOLD: शिवालापुरवा  PRED: शिवालापूर्व
SRC: deedablyuen  GOLD: डीडब्ल्यूएन  PRED: दीदब्ल्यूएं
SRC: praansangalee  GOLD: प्राणसंगली  PRED: प्रांसंगली
SRC: sainapur  GOLD: सैनपुर  PRED: सैनापुर
SRC: dimak  GOLD: दिमाक  PRED: दिमक
SRC: jyadaa  GOLD: ज़यादा  PRED: ज्यादा
SRC: jadule  GOLD: जडूले  PRED: जडुले
SRC: baatah  GOLD: बातः  PRED: बातह
SRC: viraangnaaon  GOLD: वीरांगनाओं  PRED: विरांगनाओं
SRC: udhegee  GOLD: उड़ेगी  PRED: उधेगी
SRC: wishwas  GOLD: विष्वास  PRED: विश्वास
SRC: aaryanas  GOLD: आर्यनस  PRED: आर्यानास
SRC: shamtal  GOLD: शमलात  PRED: शमतल
SRC: chatakate  GOLD: चटकाते  PRED: चटकते
SRC: adivadiyon  GOLD: आदिवादियों  PRED: अदिवादियों
SRC: kavyavidhaaon  GOLD: काव्यविधाओं  PRED: कव्यविधाओं
SRC: tinkaa  GOLD: टिंका  PRED: तिंका
SRC: neemkathana  GOLD: नीमकाथाना  PRED: नीमकथना
SRC: rajnitikaaron  GOLD: राजनीतिकारों  PRED: राजनितीकारों

Done. Files produced:
 - error_analysis_char_confusions.csv
 - error_analysis_roman_ngrams.csv
Device: cuda
Train size after length-stratified subsampling: 100000
Vocab size: 98
Total model parameters: 4,057,442

❌ Checkpoint NOT found at models/seq2seq_lstm.pth. Starting training.

--- Starting Training ---

=== Epoch 1/12 (TF: 0.50) ===
Train loss: 1.3862  Valid loss: 1.1335
Val exact_acc: 0.2710  char_acc: 0.7846  mean_edit: 1.418
Saved best model to models/seq2seq_lstm.pth.

=== Epoch 2/12 (TF: 0.47) ===
Train loss: 0.7596  Valid loss: 1.0362
Val exact_acc: 0.3426  char_acc: 0.8214  mean_edit: 1.174
Saved best model to models/seq2seq_lstm.pth.

=== Epoch 3/12 (TF: 0.45) ===
Train loss: 0.6377  Valid loss: 0.9555
Val exact_acc: 0.3764  char_acc: 0.8347  mean_edit: 1.087
Saved best model to models/seq2seq_lstm.pth.

=== Epoch 4/12 (TF: 0.43) ===
Train loss: 0.5984  Valid loss: 0.9170
Val exact_acc: 0.4040  char_acc: 0.8457  mean_edit: 1.012
Saved best model to models/seq2seq_lstm.pth.

=== Epoch 5/12 (TF: 0.41) ===
Train loss: 0.5610  Valid loss: 0.8821
Val exact_acc: 0.4214  char_acc: 0.8531  mean_edit: 0.966
Saved best model to models/seq2seq_lstm.pth.

=== Epoch 6/12 (TF: 0.39) ===
Train loss: 0.5310  Valid loss: 0.8447
Val exact_acc: 0.4324  char_acc: 0.8547  mean_edit: 0.957
Saved best model to models/seq2seq_lstm.pth.

=== Epoch 7/12 (TF: 0.37) ===
Train loss: 0.5131  Valid loss: 0.8690
Val exact_acc: 0.4362  char_acc: 0.8556  mean_edit: 0.949

=== Epoch 8/12 (TF: 0.35) ===
Train loss: 0.4916  Valid loss: 0.8630
Val exact_acc: 0.4450  char_acc: 0.8576  mean_edit: 0.938

=== Epoch 9/12 (TF: 0.33) ===
Train loss: 0.4759  Valid loss: 0.8337
Val exact_acc: 0.4392  char_acc: 0.8575  mean_edit: 0.938
Saved best model to models/seq2seq_lstm.pth.

=== Epoch 10/12 (TF: 0.32) ===
Train loss: 0.4698  Valid loss: 0.8186
Val exact_acc: 0.4394  char_acc: 0.8557  mean_edit: 0.950
Saved best model to models/seq2seq_lstm.pth.

=== Epoch 11/12 (TF: 0.30) ===
Train loss: 0.4488  Valid loss: 0.8225
Val exact_acc: 0.4510  char_acc: 0.8607  mean_edit: 0.916

=== Epoch 12/12 (TF: 0.28) ===
Train loss: 0.4382  Valid loss: 0.8197
Val exact_acc: 0.4513  char_acc: 0.8615  mean_edit: 0.913
Loaded newly trained best model from epoch 10

=== Running Final Test Evaluation ===

=== Test results ===
Exact word accuracy: 0.4229628164556962
Char accuracy (approx): 0.8513161492739231
Mean edit distance: 1.0253164556962024

Sample predictions:
SRC: maitrologist     GOLD: मैट्रोलॉजिस्ट    PRED: मेट्रोलॉजिस्ट
SRC: phwcs            GOLD: पीएचडब्ल्यूसीएस  PRED: पीएचवीसीएस
SRC: pratidwandiyon   GOLD: प्रतिद्वन्दियों  PRED: प्रतिद्वं्दियों
SRC: pratiyukti       GOLD: प्रतियुक्ति      PRED: प्रतियुक्ति
SRC: eksisatens       GOLD: एक्सिसटेंस       PRED: एकसीसेटेंस
SRC: filmnirmata      GOLD: फ़िल्मनिर्माता   PRED: फ़िल्मनिर्माता
SRC: adrgh            GOLD: अद्र्घ           PRED: अद्र्घ
SRC: ladhege          GOLD: लड़ेगे           PRED: लढ़ेगे
SRC: administresh     GOLD: एडमिनिस्ट्रेश    PRED: एडमिनिस्ट्रेश
SRC: shiwalapurwa     GOLD: शिवालापुरवा      PRED: शिवालापुरवा

--- Running LSTM Error Analysis on Test Set ---
Total test examples processed: 10112
Exact word accuracy (Recalculated): 0.4230 (5835 errors)

--- Top character confusions (gold -> predicted), sorted by frequency: ---
'ा' -> '<eps>'     count=852
'्' -> '<eps>'     count=572
'़' -> '<eps>'     count=337
'ी' -> 'ि'     count=246
'ं' -> 'न'     count=194
'ि' -> 'ी'     count=193
'ट' -> 'त'     count=192
'ू' -> 'ु'     count=172
'ड' -> 'द'     count=144
'त' -> 'ट'     count=138
'य' -> '<eps>'     count=114
'े' -> '<eps>'     count=114
'न' -> 'ं'     count=112
'ॉ' -> 'ो'     count=111
'ं' -> '<eps>'     count=107
'द' -> 'ड'     count=95
'ै' -> 'े'     count=68
'ं' -> 'ँ'     count=68
'ू' -> '<eps>'     count=67
'ण' -> 'न'     count=62
'े' -> 'ै'     count=62
'ल' -> '<eps>'     count=58
'ु' -> 'ू'     count=58
'ा' -> 'ै'     count=56
'त' -> '<eps>'     count=53
'ए' -> '<eps>'     count=51
'र' -> '<eps>'     count=48
'न' -> '<eps>'     count=47
'ठ' -> 'थ'     count=46
'ः' -> 'ह'     count=45

Summary of edit operation counts (for non-equal operations): {'replace': 6410, 'delete': 2241, 'insert': 2056}
Wrote char confusions to lstm_error_analysis_char_confusions.csv

--- Top Roman n-grams (n<= 3) by word-level error rate (min support = 20): ---
N     NGRAM SUPPORT    ERRORS     ERR_RATE  
---------------------------------------------
3     'bly  ' 27         27         1.000     
2     'sy   ' 24         23         0.958     
3     'abl  ' 32         30         0.938     
3     'wad  ' 27         25         0.926     
3     'sid  ' 27         25         0.926     
3     'dat  ' 26         24         0.923     
3     'ral  ' 24         22         0.917     
2     'ff   ' 22         20         0.909     
2     'ue   ' 33         29         0.879     
3     'yoo  ' 40         35         0.875     
3     'dab  ' 46         40         0.870     
2     'za   ' 49         42         0.857     
3     'ees  ' 35         30         0.857     
2     'sn   ' 21         18         0.857     
2     'az   ' 54         46         0.852     
3     'alm  ' 20         17         0.850     
3     'vil  ' 32         27         0.844     
2     'ob   ' 41         34         0.829     
1     'z    ' 165        136        0.824     
2     'ea   ' 95         78         0.821     
3     'ass  ' 22         18         0.818     
3     'ite  ' 22         18         0.818     
3     'ori  ' 32         26         0.812     
3     'ins  ' 32         26         0.812     
3     'ida  ' 32         26         0.812     
3     'ine  ' 42         34         0.810     
3     'mer  ' 26         21         0.808     
3     'dan  ' 81         65         0.802     
3     'ayi  ' 25         20         0.800     
3     'tte  ' 20         16         0.800     
Wrote roman ngram stats to lstm_error_analysis_roman_ngrams.csv

--- Levenshtein edit distance over test set: ---
Mean: 1.025
Median: 1.0
Max: 14

--- Sample errors (showing up to 30): ---
SRC: maitrologist     GOLD: मैट्रोलॉजिस्ट    PRED: मेट्रोलॉजिस्ट
SRC: phwcs            GOLD: पीएचडब्ल्यूसीएस  PRED: पीएचवीसीएस
SRC: pratidwandiyon   GOLD: प्रतिद्वन्दियों  PRED: प्रतिद्वं्दियों
SRC: eksisatens       GOLD: एक्सिसटेंस       PRED: एकसीसेटेंस
SRC: ladhege          GOLD: लड़ेगे           PRED: लढ़ेगे
SRC: chootie          GOLD: चूतीए            PRED: चूटीए
SRC: kursiyo          GOLD: कुर्सियॉ         PRED: कुरसियो
SRC: enseeaardablyusee  GOLD: एनसीआरडब्ल्यूसी  PRED: एनसीएअर्डब्लयूजी
SRC: shivaalapurva    GOLD: शिवालापुरवा      PRED: शिवालापूर्व
SRC: deedablyuen      GOLD: डीडब्ल्यूएन      PRED: डीडाब्ल्युएन
SRC: praansangalee    GOLD: प्राणसंगली       PRED: प्रांसंगली
SRC: sainapur         GOLD: सैनपुर           PRED: सैनापुर
SRC: dimak            GOLD: दिमाक            PRED: दिमक
SRC: jyadaa           GOLD: ज़यादा           PRED: ज्याा
SRC: jadule           GOLD: जडूले            PRED: जाडुले
SRC: viraangnaaon     GOLD: वीरांगनाओं       PRED: विरांगनाओं
SRC: udhegee          GOLD: उड़ेगी           PRED: उधेगी
SRC: wishwas          GOLD: विष्वास          PRED: विश्वास
SRC: aaryanas         GOLD: आर्यनस           PRED: आर्यनास
SRC: shamtal          GOLD: शमलात            PRED: शममाल
SRC: chatakate        GOLD: चटकाते           PRED: चटकते
SRC: dhumrapaan       GOLD: धुम्रपान         PRED: धुम्रपां
SRC: neemkathana      GOLD: नीमकाथाना        PRED: नीमकथाना
SRC: khaayal          GOLD: क़ायल            PRED: खायल
SRC: jivati           GOLD: जीवति            PRED: जीवती
SRC: rookvaaee        GOLD: रुकवाई           PRED: रूकवाई
SRC: nigmaayukt       GOLD: निगमायुक्त       PRED: निग्मायुक्त
SRC: uthaaeange       GOLD: उठाएँगे          PRED: उठाईंगे
SRC: badhane          GOLD: बड़ने            PRED: बधने
SRC: hathdharmitaa    GOLD: हठधर्मिता        PRED: हाधर्मिता

Analysis Complete. Check generated CSV files.
